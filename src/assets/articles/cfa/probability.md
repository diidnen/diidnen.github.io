###    basically对于概率，我们应该说我们是用来描述uncertainty，那么uncertainty和randomness的区别在于，我们不知道到底randomness存不存在，ok我们可以说很多东西是由于我们知道的不全所导致的，那么理论上来讲我们没有办法知道关于硬币的信息因为我们的信息不足，但是对于uncertainty，我们更多讨论的是 如果我们扔一个coin，他会朝上还是朝下，是一个考虑case的感觉

### 1.

现在先考虑随机试验，试验的结果事先不能准确地预言，具有如下三个特性：

1. 试验可以在相同的条件下重复进行
2. 试验的结果不止一个，且所有可能结果是已知的
3. 每次试验之前，不能确定哪个结果会出现
（有点像有很多个可能的实验结果，但是不知道哪个结果会出现    ）

基本概念：

1. 样本空间：随机试验的所有可能结果的集合
2. 样本点：样本空间的元素
3. 随机事件：样本空间的子集
4. 必然事件：样本空间本身
5. 不可能事件：空集

sigma代数

sigma代数 do not require us to define P in such a way that it relates usefully to any real-world situation

先补一下测度论
measure theory: space equipped with a notion of size

测度是在数学上严格定义研究大小或者概率的一个东西，我们很容易考虑在离散情形下面比如抛硬币我们可以得到正面概率是1/2，但是如果是连续的情形会有很多种不同的情况那么这个时候我们就很明显计数法行不通
在概率论中，离散事件和连续事件是两种不同的情境，直观上处理方式也不一样。
离散：通过概率质量函数 P(X = x)P(X=x) 描述。
连续：通过概率密度函数 f(x)f(x) 描述。
测度理论把离散和连续情境统一起来。概率就是一种特殊的测度（通常称为概率测度），它既能处理离散情况，也能处理连续情况，还能处理两者混合的情况。

measurable space（还没有加入测度） and measure space 

A measurable space is defined to be a space $ / omega $ of points, and a $ /sigma $ algebra of subsets of $ /omega $（换句话说就是我们定义一个sigma algebra 有两种对求并求交求余 countably finite）

然后我们定义 sigma-algebra generated by 一个集合族 is the smallest sigma algebra 然后我们说start with open sets, and apply repeatedly union/complement operations

A probability function P is a mapping P:F->R such that 
对于所有event P(E)>=0
P(H)=1 (H是sample space)
and countable additivity


independence does not give any information

mutually independent 感觉是随便取一组event他们的probability还是乘积
mutual independence is a stronger condition than pairwise independence, which is the condition that all pairs of events E_i and E_j are independent(有点感觉是因为这个pairwise是考虑的两个event，但是这个mutually independent是考虑any collection 可以参考notes的example week 1, example)
generalised multiplication law

我有这些工具可以用（conditional probability, law of total probability,bayes' Theorem(定义了given a condition then what is the probility),generalised multiplication law）

define what is called random variable(suppose H is a countable sample space.) Then a function X:H->R is called a discrete random variable

Let X be a discrete random variable taking values in the set {X_i}(which must be countable, since H is ). The function p_X(x)=P(X=x) is called the probability mass function of X

cumulative distribution function(cdf) of X(distribution function)

前面我们定义了probability now it is useful to be able to summarize a probability distribution by giving asingle typical value for a random variable(a common way of doing this is to consider the average value.)
lecture notes week2 写的 we can perform the average either on the values taken by X or directly on the sample space（个人理解为其实以下标来代表量和以sample space来表达本质是一样的）
symmetry
if X is a random variable with a symmetric pmf, and if E(X) exists, then E(X) is just the central value(intuition???)(unless E(X) exists(why is this(may not converge in countable infinite case)))
我们的工具（E(X)的表达式，以及E(Y)=aE(X)+b if Y=aX+b）
我们现在看到说如果一个X is symmetric about w then E(X)=w if it exists. 另一角度就相当于pmf在w这里balance，就很像那种moments，那么我们introduce一下这个概念

The rTh Moment of X about a is given by E((X-a)^(r)) when it exists

然后Var就是这个 second moment

Var(X)=E(X^2)-(E(x))^2
Var(X)=E(X(X-1))-E(X)(E(X)-1)
Var(X)=0 if and only if X is a constant P(X=c)=1
Y=aX+B  Var(Y)=a^2*Var(X)
考虑standard deviation是因为是想在同一个单位下面（感觉）
然后coefficient of variation of X is defined as (standard deviation)/mean(还是相同原因就是考虑一个比率)
Let $X$ be a random variable taking values $0,1,2,...$ The probability generating function of $X$ is defined as $\pi_X(z)=E(z^X)$(pgf 的intuition?)（pgf给出了X的probability的信息也就是说可以算E(X)和Var（X），然后这个z是任意的z满足这个expectation is defined）
and $\pi_X'(1)=E(X)$ and $\pi_X''(1)=E(X(X-1))$

[6个distribution
bernoulli distribution
binomial distribution(E(X)=np,var(X)=np(1-p))
geometric distribution(E(X)=1/p,Var(X)=q/p^2)
negative binomial distribution
hypergeometric distribution
poisson distribution(描述固定时间或者空间内随机事件发生的数量，我们定义the probability of one accident during some small time interval of length x time units is ax+o(x)(要保证在一个很小的时间内只有一个事情发生和binomial吻合))]（还没看完但觉得没啥意思）
uniform distribution(这个可以用来generate数字，然后也可以转换为其他distribution来generate sample（具体这个是怎么实现的）)
Exponential distribution（The exponential distribution is used to model the time between events in a Poisson process. The parameter λ>0 describes the rate at which these events occur.It tells you how frequently events happen.emoryless Property:

The exponential distribution is memoryless:
P(X > s + t | X > s) = P(X > t)
P(X>s+t∣X>s)=P(X>t)
The probability of waiting tt more time units is independent of how much time has already passed.指数分布可以描述事件发生的时间间隔，而泊松过程则刻画单位时间内事件发生的次数。为什么时间间隔服从指数分布？
泊松过程的平移不变性：

在泊松过程中，事件发生的概率只取决于时间间隔的长度，而与时间间隔起点的位置无关。
例如，无论观察的是上午 9 点到 10 点，还是下午 3 点到 4 点，事故发生的概率只取决于间隔长度 11 小时，而不是具体时间点。
内在随机性：

泊松过程的下一个事件何时发生完全随机，不依赖于前一个事件发生的具体时间。）
Gamma distribution
Beta distribution
Normal distribution

Functions of random variables(这里就是什么我们现在有一个random variable我们知道他的分布然后我们现在有一个新的random variable他和我们已知的random variable有某种函数关系（一定要是strictly monotonic function）(想一下为什么等价，以及这些东西最基本的概念))

joint probability distribution

F(x_1,x_2,.....x_n)=P(X_1<=x_1,....,X_n<=x_n) for x_1,x_2,x_3,...x_n,

the distribution function of a single random variable is called its marginal distribution function

marginal distribution function(描述了单个随机变量的分布情况)
联合分布（描述了多个随机变量的相互关系）
因�������边际分布函数可以理解为联合分布函数的边缘份(观察整个二维分布图，沿着x_1轴投影的结果就是X_1的边际分布)
联合分布函数的性质
1.F(x_1,...,x_i=-infinity,....,x_n)=0
2.F(infinity,infinity,....infinity)=1
3.联合分布函数是关于每个变量的非减函数 F(x_1,....x_n) 随着x_i增加而不减

if the random variable is discrete, the marginal distribution function is step function
2.if it is continous, then it can be described by pdf

joint probability mass function is given by p(x_1,....x_n)=P(X_1=x_1,....,X_n=x_n)(multinomial theorem)
Expectation and its properties
for correlation COV(X_1,X_2)=E(X_1*X_2)- E(X_1)*E(X_2)


E(X_1X_2)=E(X_1)E(X_2)(if X_1 and X_2 are independent random variable)

Expectation and its properties
we define the random variables to be independent if P(x_1 belongs to A and x_2 belongs to B)=p(x_1)*P(x_2)


sums of independent random variables

The easiest way to find the distribution of S_n is often via its probability or moment generating function. 

对于独立随机变量 $X_1$ 和 $X_2$，有：
$E(X_1X_2)=E(X_1)E(X_2)$

随机变量的独立性定义：
若对任意集合 $A$ 和 $B$，有 $P(X_1 \in A, X_2 \in B)=P(X_1 \in A)P(X_2 \in B)$，则称随机变量 $X_1$ 和 $X_2$ 独立。

独立随机变量的和：
对于独��随机变量的和 $S_n$，求其分布最简单的方法通常是通过其概率生成函数或矩生成函数。() 


如果X1，X2...Xn 是独立的离散的随机变量，taking non-nagative integer values, the probability generating function of their sum is the product of their probability generating functions

对于pgf we use non-negative integer values, but for mgf we can apply to both continuous and discrete random variables
mgf extend to 连续随机变量 而且mgf很好的定义了x的moment

If X_1,....X_n are(independent and identically distributed random variables with mean A and variance B^2),then E(Xn bar)=A
Var(X_n bar) = B^2/n

若 $X_1,...,X_n$ 是独立同分布的随机变量，均值为 $\mu$ 且方差为 $\sigma^2$，则：

- $E(\bar{X_n})=\mu$
- $Var(\bar{X_n})=\frac{\sigma^2}{n}$

其中 $\bar{X_n}$ 表示样本均值 $\frac{1}{n}\sum_{i=1}^n X_i$（这里我们讨论的所有这些sample聚合在一起的average的分布情况）
var衡量 the spread or dispersion around the expected value(这里是围绕着expected value的)
average有点把extreme cancels out的感觉
然后more sample就把这些原先samples里面的noise都cancels out所以就
那么根据这个var的情况，随着n增加，var减少，那么也就是说这个average的分布会越来越靠近中心点
strong law of large numbersa,
If X_1,X_2... are iid random variables with mean a, then P(limn->infinity X_n bar a)=1

强大数定律（Strong Law of Large Numbers）：
若 $X_1,X_2,...$ 是独立同分布的随机变量，均值为 $\mu$，则：

$P(\lim_{n \to \infty} \bar{X_n} = \mu)=1$
（感觉 the average will be close to the underlying mean,but it is unlikely that the average will ever be exactly equal to the underlying mean）（还没有proof）

中心极限定理（Central Limit Theorem）：
若 $X_1,X_2,...$ 是独立同分布的随机变量，均值为 $\mu$，方差为 $\sigma^2$，则：

$\lim_{n \to \infty} P\left(\frac{\sqrt{n}(\bar{X_n}-\mu)}{\sigma} \leq x\right) = \Phi(x)$

其中 $\Phi(x)$ 是标准正态分布的累积分布函数。

第6章就是怎么绘制一些sample的图像
对于continuous 或者

对于样本方差的除数是n-1
Estimation
我们要用sample的一些性质比如平均值或者方差来估计原先的分布，然后我们的任何统计量也是随机变量，那么估计量T_n会有一个概率分布，这个分布的性质就决定了T_n是否是一个好的估计量。

unbiasedness（我们希望我们的估计量要等于要顾及的参数的真实值）（无偏估计量：在多次实验中，估计量平均给出正确的值，但是我们也希望这个方差较小，我在多个重复实验中，他的期望值恰好等于被估计的参数的真实值）由于期望值在非线性转换下不会简单地遵循相同的规律(我们这里要找的是一个Expected value如果我们要找的不是一个线性的函数那么不一定能找到)
standard error(就是一个估计量的标准误差就是抽样分布的标准差)
一致性
随着样本量趋近于无穷大，估计量的值会越来越接近真实参数
一致性定理，如果估计量是真实参数的无偏估计量，而且var（估计量）->0 当n->infinity，则估计量 对于真实值是一致的
efficiency 建立在如果是unbiasedness，那么我们选取一个方差最小的，这个就是efficent


normal probability models
对于distribution（跳脱出来一点，比如啊问一个distribution 怎么被构造的，以及有哪些应用场景，为什么构造出来的能恰好被应用）

挑战假设
不同元素组合
避免先入为主


期望和当下平均不太一样

pdf和probability 就相当于 pdf描述 the relative likelihood (how likely a value is compared to other values)



A discrete random variable X has distribution function FX(·) . What is the distribution function of Y = aX + b, where a and b are constants with a > 0?
